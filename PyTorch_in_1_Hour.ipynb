{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Welcome to my tutorial of training a torch model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll create a \"batch\" of 10 data points\n",
        "N = 10\n",
        "# Each data point has 1 feature\n",
        "D_in = 1\n",
        "# The output for each data point is a single value\n",
        "D_out = 1\n",
        "\n",
        "# Create our input data X\n",
        "# Shape: (10 rows, 1 column)\n",
        "X = torch.randn(N, D_in)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Data X (first 3 rows):\n",
            " tensor([[ 2.0452],\n",
            "        [-0.5060],\n",
            "        [-1.0434]])\n",
            "\n",
            "True Labels y_true (first 3 rows):\n",
            " tensor([[ 5.0715],\n",
            "        [ 0.1660],\n",
            "        [-1.0289]])\n"
          ]
        }
      ],
      "source": [
        "# Create our true target labels y by applying the \"true\" function\n",
        "# and adding some noise for realism\n",
        "true_W = torch.tensor([[2.0]])\n",
        "true_b = torch.tensor(1.0)\n",
        "y_true = X @ true_W + true_b + torch.randn(N, D_out) * 0.1 # Add a little noise\n",
        "\n",
        "print(f\"Input Data X (first 3 rows):\\n {X[:3]}\\n\")\n",
        "print(f\"True Labels y_true (first 3 rows):\\n {y_true[:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3. The Parameters: The Model's \"Brain\"\n",
        "Now, we create the parameters W and b that our model will learn. \n",
        "We initialize them with random values. Most importantly, we set requires_grad=True to tell PyTorch's Autograd engine to start tracking them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Weight W:\n",
            " tensor([[1.1082]], requires_grad=True)\n",
            "\n",
            "Initial Bias b:\n",
            " tensor([0.9393], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Initialize our parameters with random values\n",
        "# Shapes must be correct for matrix multiplication: X(10,1) @ W(1,1) -> (10,1)\n",
        "W = torch.randn(D_in, D_out, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "print(f\"Initial Weight W:\\n {W}\\n\")\n",
        "print(f\"Initial Bias b:\\n {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4. The Implementation: From Math to Code\n",
        "Now for the main event. We translate our mathematical formula ŷ = XW + b directly into a single line of PyTorch code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of our prediction y_hat: torch.Size([10, 1])\n",
            "\n",
            "Prediction y_hat (first 3 rows):\n",
            " tensor([[ 3.2057],\n",
            "        [ 0.3786],\n",
            "        [-0.2169]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "True Labels y_true (first 3 rows):\n",
            " tensor([[ 5.0715],\n",
            "        [ 0.1660],\n",
            "        [-1.0289]])\n"
          ]
        }
      ],
      "source": [
        " # Perform the forward pass to get our first prediction\n",
        "y_hat = X @ W + b\n",
        "\n",
        "print(f\"Shape of our prediction y_hat: {y_hat.shape}\\n\")\n",
        "print(f\"Prediction y_hat (first 3 rows):\\n {y_hat[:3]}\\n\")\n",
        "print(f\"True Labels y_true (first 3 rows):\\n {y_true[:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1. Defining Error: The Loss Function\n",
        "\n",
        "We need a single number that tells us how \"wrong\" our predictions are. This is called the **Loss**. For regression, the most common loss function is the **Mean Squared Error (MSE)**.\n",
        "\n",
        "The formula is simple:\n",
        "`L = (1/N) * Σ(ŷ_i - y_i)²`\n",
        "\n",
        "In plain English: \"For every data point, find the difference between the prediction and the truth, square it, and then take the average of all these squared differences.\"\n",
        "\n",
        "Let's translate this directly into PyTorch code, using the `y_hat` from Part 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction (first 3):\n",
            " tensor([[ 3.2057],\n",
            "        [ 0.3786],\n",
            "        [-0.2169]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Truth (first 3):\n",
            " tensor([[ 5.0715],\n",
            "        [ 0.1660],\n",
            "        [-1.0289]])\n",
            "\n",
            "Loss (a single number): 1.1553044319152832\n"
          ]
        }
      ],
      "source": [
        "# y_hat is our prediction from the forward pass\n",
        "# y_true is the ground truth\n",
        "# Let's calculate the loss manually\n",
        "error = y_hat - y_true\n",
        "squared_error = error ** 2\n",
        "loss = squared_error.mean()\n",
        "\n",
        "print(f\"Prediction (first 3):\\n {y_hat[:3]}\\n\")\n",
        "print(f\"Truth (first 3):\\n {y_true[:3]}\\n\")\n",
        "print(f\"Loss (a single number): {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2. The Magic Command: `loss.backward()`\n",
        "\n",
        "This is where the magic of Autograd happens. With a single command, we tell PyTorch to send a signal backward from the `loss` through the entire computation graph it built during the forward pass.\n",
        "\n",
        "This command calculates the gradient of the `loss` with respect to every single parameter that has `requires_grad=True`. In our case, it will compute:\n",
        "*   `∂L/∂W` (the gradient of the Loss with respect to our Weight `W`)\n",
        "*   `∂L/∂b` (the gradient of the Loss with respect to our Bias `b`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss.backward()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch has now populated the `.grad` attribute for our `W` and `b` tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3. Inspecting the Result: The `.grad` Attribute\n",
        "\n",
        "The `.grad` attribute now holds the gradient for each parameter. This is the \"signal\" that tells us how to adjust our knobs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient for W (∂L/∂W):\n",
            " tensor([[-2.6305]])\n",
            "\n",
            "Gradient for b (∂L/∂b):\n",
            " tensor([0.0684])\n"
          ]
        }
      ],
      "source": [
        "# The gradients are now stored in the .grad attribute of our parameters\n",
        "print(f\"Gradient for W (∂L/∂W):\\n {W.grad}\\n\")\n",
        "print(f\"Gradient for b (∂L/∂b):\\n {b.grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **How to Interpret These Gradients:**\n",
        "\n",
        "*   **`W.grad` is -1.0185:** The negative sign is key. It means that if we were to *increase* `W`, the loss would *decrease*. The gradient points in the direction of the steepest *increase* in loss, so we'll want to move in the opposite direction.\n",
        "*   **`b.grad` is -2.0673:** Similarly, this tells us that increasing `b` will also decrease the loss.\n",
        "\n",
        "We now have everything we need to improve our model:\n",
        "1.  A way to measure error (the loss).\n",
        "2.  The exact direction to turn our parameter \"knobs\" to reduce that error (the gradients).\n",
        "\n",
        "We have completed the analysis. The final step is to actually *act* on this information—to update our weights and biases.\n",
        "\n",
        "This leads us to the heart of the training process. Let's move on to **Part 7: The Training Loop - Gradient Descent in Action**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: The Training Loop - Gradient Descent From Scratch\n",
        "\n",
        "This is the heart of the entire deep learning process. The **Training Loop** repeatedly executes the forward and backward passes, incrementally updating the model's parameters to minimize the loss. This process is called **Gradient Descent**.\n",
        "\n",
        "**The Analogy:** We're standing on a foggy mountain (the loss landscape) and want to get to the lowest valley (minimum loss). We can't see the whole map, but we can feel the slope of the ground beneath our feet (the gradients). The training loop is the process of taking a small step downhill, feeling the slope again, taking another step, and repeating until we reach the bottom.\n",
        "\n",
        "Our goal is to implement this \"step-by-step\" descent from scratch.\n",
        "\n",
        "### 7.1. The Algorithm: Gradient Descent\n",
        "\n",
        "The core update rule for gradient descent was promised in the very beginning, and now we can finally implement it:\n",
        "\n",
        "`θ_t+1 = θ_t - η * ∇_θ L`\n",
        "\n",
        "Let's translate this from math to our context:\n",
        "*   `θ`: Represents all our parameters, `W` and `b`.\n",
        "*   `η` (eta): The **learning rate**, a small number that controls how big of a step we take.\n",
        "*   `∇_θ L`: The gradient of the loss with respect to our parameters, which we now have in `W.grad` and `b.grad`.\n",
        "\n",
        "So, the update rules for our model are:\n",
        "1.  `W_new = W_old - learning_rate * W.grad`\n",
        "2.  `b_new = b_old - learning_rate * b.grad`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Parameters: W=1.803, b=-0.394\n",
            "\n",
            "Epoch 00: Loss=2.0255, W=1.805, b=-0.366\n",
            "Epoch 10: Loss=1.3684, W=1.819, b=-0.115\n",
            "Epoch 20: Loss=0.9266, W=1.835, b=0.091\n",
            "Epoch 30: Loss=0.6290, W=1.852, b=0.260\n",
            "Epoch 40: Loss=0.4283, W=1.868, b=0.399\n",
            "Epoch 50: Loss=0.2927, W=1.883, b=0.512\n",
            "Epoch 60: Loss=0.2011, W=1.897, b=0.605\n",
            "Epoch 70: Loss=0.1390, W=1.910, b=0.682\n",
            "Epoch 80: Loss=0.0970, W=1.921, b=0.745\n",
            "Epoch 90: Loss=0.0686, W=1.930, b=0.796\n",
            "Epoch 100: Loss=0.0493, W=1.938, b=0.839\n",
            "Epoch 110: Loss=0.0363, W=1.945, b=0.874\n",
            "Epoch 120: Loss=0.0274, W=1.951, b=0.902\n",
            "Epoch 130: Loss=0.0214, W=1.956, b=0.926\n",
            "Epoch 140: Loss=0.0174, W=1.961, b=0.945\n",
            "Epoch 150: Loss=0.0146, W=1.964, b=0.961\n",
            "Epoch 160: Loss=0.0127, W=1.967, b=0.974\n",
            "Epoch 170: Loss=0.0114, W=1.970, b=0.985\n",
            "Epoch 180: Loss=0.0106, W=1.972, b=0.994\n",
            "Epoch 190: Loss=0.0100, W=1.974, b=1.001\n",
            "\n",
            "Final Parameters: W=1.975, b=1.007\n",
            "True Parameters:  W=2.000, b=1.000\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 200\n",
        "\n",
        "# Let's re-initialize our random parameters\n",
        "W = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "print(f\"Starting Parameters: W={W.item():.3f}, b={b.item():.3f}\\n\")\n",
        "\n",
        "# The Training Loop\n",
        "for epoch in range(epochs):\n",
        "    ### STEP 1 & 2: Forward Pass and Loss Calculation ###\n",
        "    y_hat = X @ W + b\n",
        "    loss = torch.mean((y_hat - y_true)**2)\n",
        "\n",
        "    ### STEP 3: Backward Pass (Calculate Gradients) ###\n",
        "    loss.backward()\n",
        "\n",
        "    ### STEP 4: Update Parameters (The Gradient Descent Step) ###\n",
        "    # We wrap this in no_grad() because this is not part of the model's computation\n",
        "    with torch.no_grad():\n",
        "        W -= learning_rate * W.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    ### STEP 5: Zero the Gradients ###\n",
        "    # We must reset the gradients for the next iteration\n",
        "    W.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # Optional: Print progress\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:02d}: Loss={loss.item():.4f}, W={W.item():.3f}, b={b.item():.3f}\")\n",
        "\n",
        "print(f\"\\nFinal Parameters: W={W.item():.3f}, b={b.item():.3f}\")\n",
        "print(f\"True Parameters:  W=2.000, b=1.000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Xmf_JRJa_N8C"
      ],
      "name": "PT_Part1_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
