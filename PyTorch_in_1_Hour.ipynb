{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Welcome to my tutorial of training a torch model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll create a \"batch\" of 10 data points\n",
        "N = 10\n",
        "# Each data point has 1 feature\n",
        "D_in = 1\n",
        "# The output for each data point is a single value\n",
        "D_out = 1\n",
        "\n",
        "# Create our input data X\n",
        "# Shape: (10 rows, 1 column)\n",
        "X = torch.randn(N, D_in)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Data X (first 3 rows):\n",
            " tensor([[ 2.0452],\n",
            "        [-0.5060],\n",
            "        [-1.0434]])\n",
            "\n",
            "True Labels y_true (first 3 rows):\n",
            " tensor([[ 5.0715],\n",
            "        [ 0.1660],\n",
            "        [-1.0289]])\n"
          ]
        }
      ],
      "source": [
        "# Create our true target labels y by applying the \"true\" function\n",
        "# and adding some noise for realism\n",
        "true_W = torch.tensor([[2.0]])\n",
        "true_b = torch.tensor(1.0)\n",
        "y_true = X @ true_W + true_b + torch.randn(N, D_out) * 0.1 # Add a little noise\n",
        "\n",
        "print(f\"Input Data X (first 3 rows):\\n {X[:3]}\\n\")\n",
        "print(f\"True Labels y_true (first 3 rows):\\n {y_true[:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3. The Parameters: The Model's \"Brain\"\n",
        "Now, we create the parameters W and b that our model will learn. \n",
        "We initialize them with random values. Most importantly, we set requires_grad=True to tell PyTorch's Autograd engine to start tracking them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Weight W:\n",
            " tensor([[1.1082]], requires_grad=True)\n",
            "\n",
            "Initial Bias b:\n",
            " tensor([0.9393], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Initialize our parameters with random values\n",
        "# Shapes must be correct for matrix multiplication: X(10,1) @ W(1,1) -> (10,1)\n",
        "W = torch.randn(D_in, D_out, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "print(f\"Initial Weight W:\\n {W}\\n\")\n",
        "print(f\"Initial Bias b:\\n {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4. The Implementation: From Math to Code\n",
        "Now for the main event. We translate our mathematical formula ŷ = XW + b directly into a single line of PyTorch code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of our prediction y_hat: torch.Size([10, 1])\n",
            "\n",
            "Prediction y_hat (first 3 rows):\n",
            " tensor([[ 3.2057],\n",
            "        [ 0.3786],\n",
            "        [-0.2169]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "True Labels y_true (first 3 rows):\n",
            " tensor([[ 5.0715],\n",
            "        [ 0.1660],\n",
            "        [-1.0289]])\n"
          ]
        }
      ],
      "source": [
        " # Perform the forward pass to get our first prediction\n",
        "y_hat = X @ W + b\n",
        "\n",
        "print(f\"Shape of our prediction y_hat: {y_hat.shape}\\n\")\n",
        "print(f\"Prediction y_hat (first 3 rows):\\n {y_hat[:3]}\\n\")\n",
        "print(f\"True Labels y_true (first 3 rows):\\n {y_true[:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1. Defining Error: The Loss Function\n",
        "\n",
        "We need a single number that tells us how \"wrong\" our predictions are. This is called the **Loss**. For regression, the most common loss function is the **Mean Squared Error (MSE)**.\n",
        "\n",
        "The formula is simple:\n",
        "`L = (1/N) * Σ(ŷ_i - y_i)²`\n",
        "\n",
        "In plain English: \"For every data point, find the difference between the prediction and the truth, square it, and then take the average of all these squared differences.\"\n",
        "\n",
        "Let's translate this directly into PyTorch code, using the `y_hat` from Part 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction (first 3):\n",
            " tensor([[ 3.2057],\n",
            "        [ 0.3786],\n",
            "        [-0.2169]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Truth (first 3):\n",
            " tensor([[ 5.0715],\n",
            "        [ 0.1660],\n",
            "        [-1.0289]])\n",
            "\n",
            "Loss (a single number): 1.1553044319152832\n"
          ]
        }
      ],
      "source": [
        "# y_hat is our prediction from the forward pass\n",
        "# y_true is the ground truth\n",
        "# Let's calculate the loss manually\n",
        "error = y_hat - y_true\n",
        "squared_error = error ** 2\n",
        "loss = squared_error.mean()\n",
        "\n",
        "print(f\"Prediction (first 3):\\n {y_hat[:3]}\\n\")\n",
        "print(f\"Truth (first 3):\\n {y_true[:3]}\\n\")\n",
        "print(f\"Loss (a single number): {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2. The Magic Command: `loss.backward()`\n",
        "\n",
        "This is where the magic of Autograd happens. With a single command, we tell PyTorch to send a signal backward from the `loss` through the entire computation graph it built during the forward pass.\n",
        "\n",
        "This command calculates the gradient of the `loss` with respect to every single parameter that has `requires_grad=True`. In our case, it will compute:\n",
        "*   `∂L/∂W` (the gradient of the Loss with respect to our Weight `W`)\n",
        "*   `∂L/∂b` (the gradient of the Loss with respect to our Bias `b`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss.backward()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch has now populated the `.grad` attribute for our `W` and `b` tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3. Inspecting the Result: The `.grad` Attribute\n",
        "\n",
        "The `.grad` attribute now holds the gradient for each parameter. This is the \"signal\" that tells us how to adjust our knobs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient for W (∂L/∂W):\n",
            " tensor([[-2.6305]])\n",
            "\n",
            "Gradient for b (∂L/∂b):\n",
            " tensor([0.0684])\n"
          ]
        }
      ],
      "source": [
        "# The gradients are now stored in the .grad attribute of our parameters\n",
        "print(f\"Gradient for W (∂L/∂W):\\n {W.grad}\\n\")\n",
        "print(f\"Gradient for b (∂L/∂b):\\n {b.grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **How to Interpret These Gradients:**\n",
        "\n",
        "*   **`W.grad` is -1.0185:** The negative sign is key. It means that if we were to *increase* `W`, the loss would *decrease*. The gradient points in the direction of the steepest *increase* in loss, so we'll want to move in the opposite direction.\n",
        "*   **`b.grad` is -2.0673:** Similarly, this tells us that increasing `b` will also decrease the loss.\n",
        "\n",
        "We now have everything we need to improve our model:\n",
        "1.  A way to measure error (the loss).\n",
        "2.  The exact direction to turn our parameter \"knobs\" to reduce that error (the gradients).\n",
        "\n",
        "We have completed the analysis. The final step is to actually *act* on this information—to update our weights and biases.\n",
        "\n",
        "This leads us to the heart of the training process. Let's move on to **Part 7: The Training Loop - Gradient Descent in Action**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Xmf_JRJa_N8C"
      ],
      "name": "PT_Part1_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
