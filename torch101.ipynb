{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Welcome to my tutorial of training a torch model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Create Synthetic Data (Our \"Ground Truth\")\n",
        "# We want our model to learn the equation: y = 2x + 0.5\n",
        "W_true = torch.tensor([[2.0]])\n",
        "b_true = torch.tensor([[0.5]])\n",
        "\n",
        "# Create 100 data points, with some random noise\n",
        "X = torch.randn(100, 1)\n",
        "y_true = X @ W_true + b_true + 0.1 * torch.randn(100, 1) # Add noise\n",
        "\n",
        "print(f\"--- Data Created. Goal: Learn W=2.0 and b=0.5 ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKA_J7bdP33T"
      },
      "outputs": [],
      "source": [
        "# --- This is the code from your image ---\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate, epochs = 0.01, 100\n",
        "\n",
        "# Re-initialize parameters (These are what we will \"learn\")\n",
        "# We use randn() to start them at random values\n",
        "W = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, 1, requires_grad=True)\n",
        "\n",
        "print(f\"Initial W: {W.item():.3f}, Initial b: {b.item():.3f}\\n\")\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass and loss\n",
        "    # y_hat is our model's prediction\n",
        "    y_hat = X @ W + b\n",
        "    \n",
        "    # Calculate loss (Mean Squared Error)\n",
        "    loss = torch.mean((y_hat - y_true)**2)\n",
        "\n",
        "    # Backward pass\n",
        "    # PyTorch automatically calculates the gradients (derivatives)\n",
        "    # of the loss with respect to all tensors that have requires_grad=True (i.e., W and b)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    # We wrap this in torch.no_grad() because we are manually\n",
        "    # changing the parameters. We don't want this operation\n",
        "    # to be tracked for the next gradient calculation.\n",
        "    with torch.no_grad():\n",
        "        W -= learning_rate * W.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Zero gradients\n",
        "    # We must reset the gradients after each update.\n",
        "    # If we don't, they will accumulate on every loop.\n",
        "    W.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # --- End of your image's code ---\n",
        "\n",
        "    # Optional: Print progress\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1:3}/{epochs} | Loss: {loss.item():.4f} | W: {W.item():.3f} | b: {b.item():.3f}\")\n",
        "\n",
        "print(\"\\n--- Training Finished ---\")\n",
        "print(f\"Original W: {W_true.item():.3f} | Learned W: {W.item():.3f}\")\n",
        "print(f\"Original b: {b_true.item():.3f} | Learned b: {b.item():.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Xmf_JRJa_N8C"
      ],
      "name": "PT_Part1_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
